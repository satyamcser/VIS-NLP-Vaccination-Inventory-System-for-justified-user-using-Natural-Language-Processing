# -*- coding: utf-8 -*-
"""vaccine for justified users.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Yn1PgR0ePptyrGQZ8otBtxY98CCLDboA
"""

from google.colab import files
a = files.upload()

import pandas as pd

data = pd.read_csv("/content/vaccination_tweets.csv")
data.head(3)

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

data.describe()

data.info()

columns = []
for i in data.columns:
  columns.append(i)
columns

data.isnull().any()

data.isnull().sum()

sns.heatmap(data.isnull(),cmap='magma')

data.shape

null_value_columns = []
for i in data.columns:
  if data[i].isnull().any() == True:
    null_value_columns.append(i)
null_value_columns

np.where(data['user_location'].isnull())

np.where(data['user_description'].isnull())

np.where(data['hashtags'].isnull())

np.where(data['source'].isnull())
data['source'][1839]

null_value_columns

data['user_location'] = data["user_location"].fillna("Location Unavialable")
data['user_description'] = data["user_description"].fillna("User Description Unavialable")
data['hashtags'] = data["hashtags"].fillna("Hashtag Unavialable")
data['source'] = data["source"].fillna("Source Unavialable")

data.isnull().sum()

data.head(3)

data.shape

for i in columns:
  print("number of unique values in",i,"is = ",len(np.unique(data[i])))

plt.scatter(data['id'],data['user_followers'])

plt.scatter(data['id'],data['user_friends'])

plt.scatter(data['id'],data['user_favourites'])

plt.scatter(data['id'],data['retweets'])

plt.scatter(data['id'],data['is_retweet'])

data.head(1)

from sklearn.preprocessing import LabelEncoder
data['user_name'] = LabelEncoder().fit_transform(data['user_name'])
data['user_location'] = LabelEncoder().fit_transform(data["user_location"])
data['user_description'] = LabelEncoder().fit_transform(data["user_description"])
data['hashtags'] = LabelEncoder().fit_transform(data["hashtags"])
data['source'] = LabelEncoder().fit_transform(data["source"])

data.drop('date',axis =1,inplace=True)

data.head(2)

data.drop('user_created',axis = 1,inplace=True)

data.head(2)

data['user_verified'].replace(False,0,inplace=True)
data['user_verified'].replace(True,1,inplace=True)
data['user_verified'] =data['user_verified'].astype(int)

data.drop('is_retweet',axis = 1,inplace=True)

data.head(2)

data.drop('id',axis = 1,inplace=True)

data.head(1)

sns.countplot(x='user_name',hue='user_verified',data=data)

plt.plot(data["user_location"],color='b')
plt.show()

plt.plot(data['user_followers'],color='g')

sns.violinplot(x='user_location',hue='user_verified',data=data)

take = []
drop = []
for j in data.columns:
  if j == 'text' or j == 'user_verified':
    take.append(j)
  else:
    drop.append(j)

drop

data.drop(drop,axis=1,inplace=True)

data.head()

x = data['text']
y = data['user_verified']

sns.countplot(x = 'user_verified',data=data)

verified_user = []
not_verified_user = []
for i in data['user_verified']:
  if i == 1:
    verified_user.append(i)
  else:
    not_verified_user.append(i)

print("percentage of verified user = ",((len(verified_user)/len(data['user_verified']))*100))

print("percentage of not verified user = ",((len(not_verified_user)/len(data['user_verified']))*100))

from tensorflow import keras
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences

tokenizer = Tokenizer(10000,lower=True)
tokenizer.fit_on_texts(x)

sequence = tokenizer.texts_to_sequences(x)
final_input = pad_sequences(sequence,maxlen=200,padding='pre')

final_output = np.array(y)

from keras.models import Sequential
from keras.layers import Bidirectional,LSTM,Dense,Embedding

model = Sequential()
model.add(
    Embedding(
        input_dim=10000,
        output_dim=120,
        input_length=200,
    ),)
model.add(
    Bidirectional(LSTM(64, return_sequences=True))
)
model.add(Bidirectional(LSTM(32)))
model.add(Dense(1))

model.summary()

model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])

pred = model.fit(final_input,final_output,epochs=5,batch_size=32)

model.save("vaccine delivery.h5")

plt.plot(pred.history['loss'],label='loss')
plt.plot(pred.history['accuracy'],label='accuracy')
plt.legend(loc='best')
plt.show()

def verification(details):
  sentence_list = []
  sentence_list.append(details)
  fit = tokenizer.texts_to_sequences(sentence_list)
  sequence = pad_sequences(fit,maxlen=80,padding='pre')
  output = model.predict_classes(sequence)
  if output.all() == 1:
    print("Verified User")
  else:
    print("User is not verified")

verification("While the world has been on the wrong side of history this year, hopefully, the biggest vaccination effort we've evâ€¦ https://t.co/dlCHrZjkhm")